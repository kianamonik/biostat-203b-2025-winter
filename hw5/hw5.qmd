---
title: "Biostat 203B Homework 5"
subtitle: Due Mar 20 @ 11:59PM
author: "Kiana Mohammadinik and 205928003"
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: false
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
  pdf:
    number-sections: true
    toc: true
    toc-depth: 3
---

## Predicting ICU duration
```{r}
library(readr)
library(dplyr)
library(recipes)
library(tidymodels)
library(stacks)
library(finetune)  
library(doParallel)
library(dials) 
library(vip)
library(future)
library(tune)
library(ROCR)
library(missRanger)
library(tidyverse)
library(e1071)
library(bonsai)
library(kernlab)
library(pROC)
```

Using the ICU cohort `mimiciv_icu_cohort.rds` you built in Homework 4, develop at least three machine learning approaches (logistic regression with enet regularization, random forest, boosting, SVM, MLP, etc) plus a model stacking approach for predicting whether a patient's ICU stay will be longer than 2 days. You should use the `los_long` variable as the outcome. You algorithms can use patient demographic information (gender, age at ICU `intime`, marital status, race), ICU admission information (first care unit), the last lab measurements before the ICU stay, and first vital measurements during ICU stay as features. You are welcome to use any feature engineering techniques you think are appropriate; but make sure to not use features that are not available at an ICU stay's `intime`. For instance, `last_careunit` cannot be used in your algorithms. 

1. Data preprocessing and feature engineering.
```{r}
# Load dataset
mimiciv_icu_cohort <- read_rds("mimic_icu_cohort.rds")

# Create binary outcome variable
mimiciv_icu_cohort <- mimiciv_icu_cohort |> 
  mutate(los_long = as.factor(los > 2))

# Sort dataset
mimiciv_icu_cohort <- mimiciv_icu_cohort |> 
  arrange(subject_id, hadm_id, stay_id)

# Drop variables with >7000 NAs to improve imputation
missing_summary <- colSums(is.na(mimiciv_icu_cohort))
valid_columns <- names(missing_summary[missing_summary < 7000])
mimiciv_icu_cohort <- mimiciv_icu_cohort |> select(all_of(valid_columns))

# Select relevant features
selected_features <- c(
  "los_long", "gender", "age_at_intime", "race", "first_careunit",
  "non_invasive_blood_pressure_systolic", "temperature_fahrenheit",
  "respiratory_rate", "non_invasive_blood_pressure_diastolic", "heart_rate",
  "admission_type", "admission_location", "insurance", "wbc", "hematocrit", 
  "language", "admit_provider_id"
)

mimiciv_icu_cohort <- mimiciv_icu_cohort |> select(all_of(selected_features))

# Convert categorical variables to factors
mimiciv_icu_cohort <- mimiciv_icu_cohort |> 
  mutate(across(where(is.character), as.factor))

# Convert all integer columns to numeric
mimiciv_icu_cohort <- mimiciv_icu_cohort |> 
  mutate(across(where(is.integer), as.numeric))

# Ensure `los_long` has no missing values
mimiciv_icu_cohort <- mimiciv_icu_cohort |> drop_na(los_long)

# Function to replace outliers with NA using IQR
remove_outliers <- function(x) {
  if (is.numeric(x)) {
    qnt <- quantile(x, probs = c(0.25, 0.75), na.rm = TRUE)
    IQR_value <- IQR(x, na.rm = TRUE)
    x[x < (qnt[1] - 1.5 * IQR_value)] <- NA
    x[x > (qnt[2] + 1.5 * IQR_value)] <- NA
  }
  return(x)
}

# Apply outlier removal to numeric columns
mimiciv_icu_cohort <- mimiciv_icu_cohort |> 
  mutate(across(where(is.numeric), remove_outliers))

# Impute missing values using missRanger
mimiciv_icu_cohort <- missRanger(
  mimiciv_icu_cohort, 
  num.trees = 100, 
  maxiter = 5, 
  verbose = 2
)

# Normalize numeric features
mimiciv_icu_cohort <- mimiciv_icu_cohort |> 
  mutate(across(where(is.numeric), scale))

# Identify skewed variables
numeric_features <- mimiciv_icu_cohort %>% select(where(is.numeric))
skewed_vars <- apply(numeric_features, 2, function(x) skewness(x, na.rm = TRUE))

# Apply log transformation to highly skewed features
skewed_features <- names(skewed_vars[abs(skewed_vars) > 1])
mimiciv_icu_cohort <- mimiciv_icu_cohort %>%
  mutate(across(all_of(skewed_features), log1p))

glimpse(mimiciv_icu_cohort)
```


2. Partition data into 50% training set and 50% test set. Stratify partitioning according to `los_long`. For grading purpose, sort the data by `subject_id`, `hadm_id`, and `stay_id` and use the seed `203` for the initial data split. Below is the sample code.
```{r}
set.seed(203)

data_split <- initial_split(
  mimiciv_icu_cohort, 
  # stratify by los_long
  strata = "los_long", 
  prop = 0.5
  )

train_data <- training(data_split)
test_data  <- testing(data_split)
```

3. Train and tune the models using the training set.
```{r}
# Setup Parallel Processing
plan(multisession, workers = parallel::detectCores() - 1)

# Logistic Regression with Elastic Net Regularization
log_spec <- logistic_reg(
  penalty = tune(),
  mixture = tune()
) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

log_wf <- workflow() %>%
  add_model(log_spec) %>%
  add_formula(los_long ~ .)  

# Random Forest Model
rf_spec <- rand_forest(
  mtry = tune(),
  min_n = tune(),
  trees = 500 
) %>%
  set_engine("ranger", importance = "permutation") %>% 
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_spec) %>%
  add_formula(los_long ~ .)  

# Boosted Trees (XGBoost)
xgb_spec <- boost_tree(
  trees = 500, 
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = tune(),
  sample_size = tune(),
  min_n = tune()
) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

xgb_wf <- workflow() %>%
  add_model(xgb_spec) %>%
  add_formula(los_long ~ .) 

lightgbm_spec <- boost_tree(
  trees = 1000,
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = tune(),
  sample_size = tune(),
  min_n = tune()
) %>%
  set_engine("lightgbm") %>%
  set_mode("classification")

lightgbm_wf <- workflow() %>%
  add_model(lightgbm_spec) %>%
  add_formula(los_long ~ .) 

# Cross-validation Setup 
set.seed(203)
folds <- vfold_cv(train_data, v = 10, strata = los_long)  

# Tune Each Model with Racing
set.seed(203)
log_res <- tune_race_anova(
  log_wf,
  resamples = folds,
  param_info = extract_parameter_set_dials(log_spec), 
  metrics = metric_set(roc_auc, accuracy),
  control = control_race(verbose_elim = TRUE)
)

set.seed(203)
rf_res <- tune_race_anova(
  rf_wf,
  resamples = folds,
  param_info = extract_parameter_set_dials(rf_spec), 
  metrics = metric_set(roc_auc, accuracy),
  control = control_race(verbose_elim = TRUE)
)

set.seed(203)
xgb_res <- tune_race_anova(
  xgb_wf,
  resamples = folds,
  param_info = extract_parameter_set_dials(xgb_spec), 
  metrics = metric_set(roc_auc, accuracy),
  control = control_race(verbose_elim = TRUE)
)

lightgbm_res <- tune_race_anova(
  lightgbm_wf,
  resamples = folds,
  param_info = extract_parameter_set_dials(lightgbm_spec), 
  metrics = metric_set(roc_auc, accuracy),
  control = control_race(verbose_elim = TRUE)
)

plan(sequential) 


# Stacking the Models
final_log_fit <- log_wf %>%
  finalize_workflow(select_best(log_res, metric = "roc_auc")) %>%
  fit(train_data)

final_rf_fit <- rf_wf %>%
  finalize_workflow(select_best(rf_res, metric = "roc_auc")) %>%
  fit(train_data)

final_xgb_fit <- xgb_wf %>%
  finalize_workflow(select_best(xgb_res, metric = "roc_auc")) %>%
  fit(train_data)

# Feature Importance for LightGBM
final_lightgbm_fit <- lightgbm_wf %>%
  finalize_workflow(select_best(lightgbm_res, metric = "roc_auc")) %>%
  fit(train_data)

vip(extract_fit_engine(final_lightgbm_fit), num_features = 10)

positive_class <- levels(train_data$los_long)[2]
prob_col <- paste0(".pred_", positive_class)

train_preds <- train_data %>%
  bind_cols(
    predict(final_log_fit, new_data = train_data, type = "prob") %>%
      rename(log_pred = !!sym(prob_col)),
    predict(final_rf_fit, new_data = train_data, type = "prob") %>% 
      rename(rf_pred = !!sym(prob_col)),
    predict(final_xgb_fit, new_data = train_data, type = "prob") %>%
      rename(xgb_pred = !!sym(prob_col)),
    predict(final_lightgbm_fit, new_data = train_data, type = "prob") %>%
      rename(lightgbm_pred = !!sym(prob_col))
  ) %>%
  select(los_long, log_pred, rf_pred, xgb_pred, lightgbm_pred)

stacking_spec <- logistic_reg(penalty = 0.01, mixture = 0.5) %>% 
  set_engine("glmnet") %>%
  set_mode("classification")

stacking_wf <- workflow() %>%
  add_model(stacking_spec) %>%
  add_formula(los_long ~ log_pred + rf_pred + xgb_pred + lightgbm_pred)

stacking_fit <- stacking_wf %>% fit(train_preds)
```


4. Compare model classification performance on the test set. Report both the area under ROC curve and accuracy for each machine learning algorithm and the model stacking. Interpret the results. What are the most important features in predicting long ICU stays? How do the models compare in terms of performance and interpretability?

```{r}
# Function to evaluate a fitted model on the test set
evaluate_model <- function(model_fit, test_data, model_name) {
  pred_prob <- predict(model_fit, new_data = test_data, type = "prob")
  pred_class <- predict(model_fit, new_data = test_data, type = "class")
  
  preds <- bind_cols(test_data, pred_prob, pred_class)
  
  positive_class <- levels(test_data$los_long)[2]
  prob_col <- paste0(".pred_", positive_class)
  
  roc_auc_metric <- roc_auc(preds, truth = los_long, !!sym(prob_col))
  accuracy_metric <- accuracy(preds, truth = los_long, estimate = .pred_class)
  
  metrics <- bind_rows(roc_auc_metric, accuracy_metric) %>% 
    mutate(Model = model_name)
  
  return(metrics)
}

# Evaluate Models
log_metrics <- evaluate_model(final_log_fit, test_data, "Logistic Regression")
rf_metrics <- evaluate_model(final_rf_fit, test_data, "Random Forest")
xgb_metrics <- evaluate_model(final_xgb_fit, test_data, "XGBoost")
lightgbm_metrics <- evaluate_model(final_lightgbm_fit, test_data, "LightGBM")

# Generate test set predictions from base models
test_preds <- test_data %>%
  bind_cols(
    predict(final_log_fit, new_data = test_data, type = "prob") %>%
      rename(log_pred = !!sym(prob_col)),
    predict(final_rf_fit, new_data = test_data, type = "prob") %>%
      rename(rf_pred = !!sym(prob_col)),
    predict(final_xgb_fit, new_data = test_data, type = "prob") %>% 
      rename(xgb_pred = !!sym(prob_col)),
    predict(final_lightgbm_fit, new_data = test_data, type = "prob") %>%
      rename(lightgbm_pred = !!sym(prob_col))
  ) %>%
  select(los_long, log_pred, rf_pred, xgb_pred, lightgbm_pred)

# Make predictions using the stacked model
stacked_test_preds <- predict(stacking_fit, new_data = test_preds,
                              type = "prob")

# Identify the correct probability column 
stacked_prob_col <- 
  colnames(stacked_test_preds)[str_detect(colnames(stacked_test_preds),
                                          "^.pred_")][1]

# Add the true labels to the stacked predictions
stacked_test_preds <- bind_cols(test_preds %>% 
                                  select(los_long), stacked_test_preds)

# Rename the probability column for clarity
stacked_test_preds <- stacked_test_preds %>%
  rename(.pred_prob = !!sym(stacked_prob_col))

# Add predicted class labels
stacked_test_preds$.pred_class <- ifelse(stacked_test_preds$.pred_prob > 0.5,
                                         "TRUE", "FALSE")
stacked_test_preds$.pred_class <- as.factor(stacked_test_preds$.pred_class)

# Evaluate the stacked model
stack_metrics <- bind_rows(
  roc_auc(stacked_test_preds, truth = los_long, .pred_prob) %>% 
    mutate(Model = "Stacked Model"),
  accuracy(stacked_test_preds, truth = los_long, estimate = .pred_class) %>% 
    mutate(Model = "Stacked Model")
)

# Print the stacked model metrics
print(stack_metrics)

# Combine all results
all_metrics <- bind_rows(log_metrics, rf_metrics, xgb_metrics, stack_metrics,
                         lightgbm_metrics)

# Interpret the results and identify important features
cat("\nFeature Importance for Random Forest:\n")
vip(extract_fit_engine(final_rf_fit), num_features = 10)

cat("\nModel Performance Comparison:\n")
print(all_metrics)
```
The performance metrics show that the stacked model has the highest ROC AUC (0.611) among all models, but its accuracy (0.422) is lower than the base models. This suggests that the stacked model is better at ranking predictions (higher ROC AUC) but struggles with classification accuracy.