---
title: "Biostat 203B Homework 1"
subtitle: Due Jan 24, 2025 @ 11:59PM
author: Kiana Mohammadinik and UID 205928003
format:
  pdf:
    theme: cosmo
    embed-resources: true
    number-sections: false
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
    link-external-icon: true
    link-external-newwindow: true
---

Display machine information for reproducibility:

```{r}
#| eval: true
sessionInfo()
```

## Q1. Git/GitHub

**No handwritten homework reports are accepted for this course.** We work with Git and GitHub. Efficient and abundant use of Git, e.g., frequent and well-documented commits, is an important criterion for grading your homework.

1.  Apply for the [Student Developer Pack](https://education.github.com/pack) at GitHub using your UCLA email. You'll get GitHub Pro account for free (unlimited public and private repositories).

2.  Create a **private** repository `biostat-203b-2025-winter` and add `Hua-Zhou` and TA team (`Tomoki-Okuno` for Lec 1; `parsajamshidian` and `BowenZhang2001` for Lec 82) as your collaborators with write permission.

3.  Top directories of the repository should be `hw1`, `hw2`, ... Maintain two branches `main` and `develop`. The `develop` branch will be your main playground, the place where you develop solution (code) to homework problems and write up report. The `main` branch will be your presentation area. Submit your homework files (Quarto file `qmd`, `html` file converted by Quarto, all code and extra data sets to reproduce results) in the `main` branch.

4.  After each homework due date, course reader and instructor will check out your `main` branch for grading. Tag each of your homework submissions with tag names `hw1`, `hw2`, ... Tagging time will be used as your submission time. That means if you tag your `hw1` submission after deadline, penalty points will be deducted for late submission.

5.  After this course, you can make this repository public and use it to demonstrate your skill sets on job market.

**Solution:** done

## Q2. Data ethics training

This exercise (and later in this course) uses the [MIMIC-IV data v3.1](https://physionet.org/content/mimiciv/3.1/), a freely accessible critical care database developed by the MIT Lab for Computational Physiology. Follow the instructions at <https://mimic.mit.edu/docs/gettingstarted/> to (1) complete the CITI `Data or Specimens Only Research` course and (2) obtain the PhysioNet credential for using the MIMIC-IV data. Display the verification links to your completion report and completion certificate here. **You must complete Q2 before working on the remaining questions.** (Hint: The CITI training takes a few hours and the PhysioNet credentialing takes a couple days; do not leave it to the last minute.)

**Solution:** Here is the [Completion Report](https://www.citiprogram.org/verify/?k603e4898-1000-48d2-96e6-aab97e6d3318-67319231) and [Completion Certificate](https://www.citiprogram.org/verify/?wb61d12d4-64fd-4feb-b443-0ff856f50eb7-67319231) of my CITI training.

## Q3. Linux Shell Commands

1.  Make the MIMIC-IV v3.1 data available at location `~/mimic`. The output of the `ls -l ~/mimic` command should be similar to the below (from my laptop).

```{bash}
#| eval: true
# content of mimic folder
ls -l ~/mimic/
```

Refer to the documentation <https://physionet.org/content/mimiciv/3.1/> for details of data files. Do **not** put these data files into Git; they are big. Do **not** copy them into your directory. Do **not** decompress the gz data files. These create unnecessary big files and are not big-data-friendly practices. Read from the data folder `~/mimic` directly in following exercises.

Use Bash commands to answer following questions.

**Solution:** I downloaded the MIMIC IV v3.1 data and it's available under `~/mimic` folder as requested.

2.  Display the contents in the folders `hosp` and `icu` using Bash command `ls -l`. Why are these data files distributed as `.csv.gz` files instead of `.csv` (comma separated values) files? Read the page <https://mimic.mit.edu/docs/iv/> to understand what's in each folder.

**Solution:** Here is the content of `hosp` folder

```{bash}
ls -l ~/mimic/hosp/
```

and content of the `icu` folder

```{bash}
ls -l ~/mimic/icu/
```

These data were distributed as `gz` file because `.csv.gz` files effectively compress big data files such as this and reduce storage space needed for them by decreasing the file size.

3.  Briefly describe what Bash commands `zcat`, `zless`, `zmore`, and `zgrep` do.

**Solution:** `zcat` is used to view the contents of a compressed file without having to uncompress the file. `zless` allows us to view the contents of compressed files withouth having to decompress them first. `zmore` uncompresses files and displays them one screenful at a time. `zgrep` command is used to search within compressed files without explicitly decompressing them first.


4.  (Looping in Bash) What's the output of the following bash script?

```{bash}
#| eval: True
for datafile in ~/mimic/hosp/{a,l,pa}*.gz
do
  ls -l $datafile
done
```
**Solution:** This loop iterates through all files in the ~/mimic/hosp/ directory that start with `a`, `l`, or `pa` and end with `.gz` and list the details such as size and permissions of each file that matches. In this scenario, the output turned out to be the admissions, labevents, and patients files that matched and their details were listed.

Display the number of lines in each data file using a similar loop. (Hint: combine linux commands `zcat <` and `wc -l`.)

Displaying the number of lines in each data file
```{bash}
for datafile in ~/mimic/hosp/{a,l,pa}*.gz
do
  echo -n "$datafile: "
  zcat < "$datafile" | wc -l
done
```


5.  Display the first few lines of `admissions.csv.gz`. How many rows are in this data file, excluding the header line? Each `hadm_id` identifies a hospitalization. How many hospitalizations are in this data file? How many unique patients (identified by `subject_id`) are in this data file? Do they match the number of patients listed in the `patients.csv.gz` file? (Hint: combine Linux commands `zcat <`, `head`/`tail`, `awk`, `sort`, `uniq`, `wc`, and so on.)

**Solution:** Here's the first few lines of `admissions.csv.gz`
```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz | head
```
The number of rows in this data file, excluding the header line, is
```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz | tail -n +2 | wc -l
```
The number of hospitalizations in this data file is
```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz |
tail -n +2 |
cut -d, -f2 |
sort |
uniq | 
wc -l
```
the same as the number of rows in the file. 

Peek the first few lines of `patients.csv.gz`:
```{bash}
zcat < ~/mimic/hosp/patients.csv.gz | head
```

The number of unique patients in this data file is
```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz |
tail -n +2 |
awk -F, '{print $1}' |
sort |
uniq |
wc -l
```
which is less than the number of paients listed in the `patients.csv.gz` file.
```{bash}
zcat < ~/mimic/hosp/patients.csv.gz |
tail -n +2 |
awk -F, '{print $1}' |
sort |
uniq |
wc -l
```


6.  What are the possible values taken by each of the variable `admission_type`, `admission_location`, `insurance`, and `ethnicity`? Also report the count for each unique value of these variables in decreasing order. (Hint: combine Linux commands `zcat`, `head`/`tail`, `awk`, `uniq -c`, `wc`, `sort`, and so on; skip the header line.)

**Solution:**

Figuring out the column number for each variable
```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz |
head -n 1 |
tr ',' '\n' |
nl
```
The count for each unique value of `admission_type` in decreasing order
```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz |
tail -n +2 |
awk -F, '{print $6}' |
sort |
uniq -c |
sort -nr
```
The count for each unique value of `admission_location` in decreasing order
```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz |
tail -n +2 |
awk -F, '{print $8}' |
sort |
uniq -c |
sort -nr
```
The count for each unique value of `insurance` in decreasing order
```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz |
tail -n +2 |
awk -F, '{print $10}' |
sort |
uniq -c |
sort -nr
```
The count for each unique value of `ethnicity` in decreasing order
```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz |
tail -n +2 |
awk -F, '{print $13}' |
sort |
uniq -c |
sort -nr
```

7.  The `icustays.csv.gz` file contains all the ICU stays during the study period. How many ICU stays, identified by `stay_id`, are in this data file? How many unique patients, identified by `subject_id`, are in this data file?

**Solution:**
Figuring out the column number for `stay_id` and `subject_id`
```{bash}
zcat < ~/mimic/icu/icustays.csv.gz |
head -n 1 |
tr ',' '\n' |
nl
```
The number of ICU stays identified by `stay_id`
```{bash}
zcat < ~/mimic/icu/icustays.csv.gz |
tail -n +2 |
awk -F, '{print $3}' |
sort |
uniq |
wc -l
```

The number of unique patients identified by `subject_id`
```{bash}
zcat < ~/mimic/icu/icustays.csv.gz |
tail -n +2 |
awk -F, '{print $1}' |
sort |
uniq |
wc -l
```

8.  *To compress, or not to compress. That's the question.* Let's focus on the big data file `labevents.csv.gz`. Compare compressed gz file size to the uncompressed file size. Compare the run times of `zcat < ~/mimic/labevents.csv.gz | wc -l` versus `wc -l labevents.csv`. Discuss the trade off between storage and speed for big data files. (Hint: `gzip -dk < FILENAME.gz > ./FILENAME`. Remember to delete the large `labevents.csv` file after the exercise.)

**Solution:**
```{bash}
ls -lh ~/mimic/hosp/labevents.csv.gz
gzip -dk ~/mimic/hosp/labevents.csv.gz
ls -lh ~/mimic/hosp/labevents.csv
```
The runtime for `zcat < ~/mimic/labevents.csv.gz | wc -l` is
```{bash}
time zcat < ~/mimic/hosp/labevents.csv.gz | wc -l
```
The runtime for `wc -l labevents.csv`
```{bash}
time wc -l ~/mimic/hosp/labevents.csv
```
Deleting the `labevents.csv` file
```{bash}
rm ~/mimic/hosp/labevents.csv
```
The difference between the two runtimes shows that even though compressed files save significant storage space, they also require decompression for usage which can be slower than a regular file that doesn't need decompression.



## Q4. Who's popular in Price and Prejudice

1.  You and your friend just have finished reading *Pride and Prejudice* by Jane Austen. Among the four main characters in the book, Elizabeth, Jane, Lydia, and Darcy, your friend thinks that Darcy was the most mentioned. You, however, are certain it was Elizabeth. Obtain the full text of the novel from <http://www.gutenberg.org/cache/epub/42671/pg42671.txt> and save to your local folder.

```{bash}
#| eval: True
wget -nc http://www.gutenberg.org/cache/epub/42671/pg42671.txt
```

Explain what `wget -nc` does. Do **not** put this text file `pg42671.txt` in Git. Complete the following loop to tabulate the number of times each of the four characters is mentioned using Linux commands.

**Solution:** `wget -nc` has two parts where `wget` is used to downlaod files from the web and `-nc` which stands for no-clobber prevents the code from overwriting an already existing file by not allowing it to download again if the file already exists.

```{bash}
#| eval: True
wget -nc http://www.gutenberg.org/cache/epub/42671/pg42671.txt
for char in Elizabeth Jane Lydia Darcy
do
  echo $char:
  # some bash commands here
done
```

2.  What's the difference between the following two commands?

```{bash}
#| eval: True
echo 'hello, world' > test1.txt
```

and

```{bash}
#| eval: True
echo 'hello, world' >> test2.txt
```

**Solution:** the command with `>` operator overwrites the file test1.txt with the text hello, world while the command with the `>>` appends the given text hello, world to the file test2.txt. With the `>>` operator, if the file test2.txt already exists, the text hello, world will be added to the end of the file instead of overwriting it.

3.  Using your favorite text editor (e.g., `vi`), type the following and save the file as `middle.sh`:

```{bash eval=FALSE}
#!/bin/sh
# Select lines from the middle of a file.
# Usage: bash middle.sh filename end_line num_lines
head -n "$2" "$1" | tail -n "$3"
```

Using `chmod` to make the file executable by the owner, and run

```{bash}
#| eval: True
./middle.sh pg42671.txt 20 5
```

Explain the output. Explain the meaning of `"$1"`, `"$2"`, and `"$3"` in this shell script. Why do we need the first line of the shell script?

**Solution:** The output shows the date May 9, 2013, the format of the ebook, and it also shows that the language of ht ebook is English. `"$1"` refers to the first argument  which is the filename. `"$2"` refers to the second argument which is the number of lines from the start. `"$3"` refers to the third argument which is the number of lines to extract from the end of the previously selected lines. This resulted in the last 5 lines from the first 20 lines in the Pride and Prejudice text to be extracted. We need the first line in the shell script becuase it specifies which interpreter to use when executing the script and without it the script might not run properly.


## Q5. More fun with Linux

Try following commands in Bash and interpret the results: `cal`, `cal 2025`, `cal 9 1752` (anything unusual?), `date`, `hostname`, `arch`, `uname -a`, `uptime`, `who am i`, `who`, `w`, `id`, `last | head`, `echo {con,pre}{sent,fer}{s,ed}`, `time sleep 5`, `history | tail`.

**Solution:** 
```{bash}
cal
```

```{bash}
cal 2025
```

```{bash}
cal 9 1752
```
The unusual occurance in the ouput for `cal 9 1752` is that the calender skips sept 3-13 in 1752. Upon further research, I found the reason for this is that 1752 was the year that in the British Empire adopted the Gregorian calendar and 11 days were dropped.
```{bash}
date
```

```{bash}
hostname
```

```{bash}
arch
```

```{bash}
uname -a
```

```{bash}
uptime
```

```{bash}
who am i
```

```{bash}
who
```

```{bash}
w
```

```{bash}
id
```

```{bash}
last | head
```

```{bash}
echo {con,pre}{sent,fer}{s,ed}
```

```{bash}
time sleep 5
```

```{bash}
history | tail
```


Done. 

## Q6. Book

1.  Git clone the repository <https://github.com/christophergandrud/Rep-Res-Book> for the book *Reproducible Research with R and RStudio* to your local machine. Do **not** put this repository within your homework repository `biostat-203b-2025-winter`.

2.  Open the project by clicking `rep-res-3rd-edition.Rproj` and compile the book by clicking `Build Book` in the `Build` panel of RStudio. (Hint: I was able to build `git_book` and `epub_book` directly. For `pdf_book`, I needed to add a line `\usepackage{hyperref}` to the file `Rep-Res-Book/rep-res-3rd-edition/latex/preabmle.tex`.)

The point of this exercise is (1) to obtain the book for free and (2) to see an example how a complicated project such as a book can be organized in a reproducible way. Use `sudo apt install PKGNAME` to install required Ubuntu packages and `tlmgr install PKGNAME` to install missing TexLive packages.

For grading purpose, include a screenshot of Section 4.1.5 of the book here.

**Solution:** Here is a screenshot of the book.
![](bookscreenshot.jpg)
